{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Delivery Time Prediction Model\n",
    "## تحليل بيانات الاستلام والتوصيل وتوقع وقت التوصيل المحسن\n",
    "\n",
    "**الهدف:** توقع ETA (Estimated Time of Arrival) بدقة عالية\n",
    "ETA = وقت التوصيل - وقت الاستلام\n",
    "\n",
    "**التحسينات:**\n",
    "- تنظيف البيانات بشكل أفضل\n",
    "- إزالة الـ outliers\n",
    "- تحليل استكشافي شامل\n",
    "- هندسة خصائص متقدمة\n",
    "- تحسين النماذج"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.linear_model import Ridge, ElasticNet, LassoCV\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    IsolationForest,\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import catboost as cb\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plotting style\n",
    "try:\n",
    "    plt.style.use(\"seaborn-v0_8\")\n",
    "except:\n",
    "    plt.style.use(\"seaborn\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA LOADING AND EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load pickup and delivery data from all cities with better error handling\"\"\"\n",
    "    cities = [\"cq\", \"sh\", \"hz\", \"jl\", \"yt\"]\n",
    "    pickup_dfs = []\n",
    "    delivery_dfs = []\n",
    "\n",
    "    for city in cities:\n",
    "        try:\n",
    "            # Load pickup data\n",
    "            pickup_df = pd.read_csv(f\"Pickup Five Cities Datasets/pickup_{city}.csv\")\n",
    "            pickup_df[\"source_city\"] = city\n",
    "            pickup_dfs.append(pickup_df)\n",
    "\n",
    "            # Load delivery data\n",
    "            delivery_df = pd.read_csv(\n",
    "                f\"Delivery Five Cities Datasets/delivery_{city}.csv\"\n",
    "            )\n",
    "            delivery_df[\"source_city\"] = city\n",
    "            delivery_dfs.append(delivery_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    if not pickup_dfs or not delivery_dfs:\n",
    "        raise ValueError(\"لم يتم تحميل أي بيانات بنجاح\")\n",
    "\n",
    "    # Combine data\n",
    "    df_pickup = pd.concat(pickup_dfs, ignore_index=True)\n",
    "    df_delivery = pd.concat(delivery_dfs, ignore_index=True)\n",
    "\n",
    "    return df_pickup, df_delivery\n",
    "\n",
    "def comprehensive_data_exploration(df_pickup, df_delivery):\n",
    "    \"\"\"تحليل شامل للبيانات\"\"\"\n",
    "    # This function originally had print statements for exploration.\n",
    "    # In a notebook, we would typically use df.info(), df.head(), df.describe() directly in cells.\n",
    "    return df_pickup, df_delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ADVANCED DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_data_preparation(df_pickup, df_delivery):\n",
    "    \"\"\"تحضير البيانات المتقدم مع تنظيف شامل\"\"\"\n",
    "    pickup_clean = df_pickup.copy()\n",
    "    delivery_clean = df_delivery.copy()\n",
    "\n",
    "    def parse_datetime_flexible(date_series, formats=None):\n",
    "        if formats is None:\n",
    "            formats = [\n",
    "                \"%m-%d %H:%M:%S\",\n",
    "                \"%Y-%m-%d %H:%M:%S\",\n",
    "                \"%d-%m-%Y %H:%M:%S\",\n",
    "                \"%m/%d %H:%M:%S\",\n",
    "                \"%Y/%m/%d %H:%M:%S\",\n",
    "            ]\n",
    "\n",
    "        parsed_series = pd.NaT\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                parsed_series = pd.to_datetime(date_series, format=fmt, errors=\"coerce\")\n",
    "                valid_count = parsed_series.notna().sum()\n",
    "                if valid_count > 0:\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        return parsed_series\n",
    "\n",
    "    pickup_clean[\"pickup_time\"] = parse_datetime_flexible(pickup_clean[\"pickup_time\"])\n",
    "    pickup_clean[\"accept_time\"] = parse_datetime_flexible(pickup_clean[\"accept_time\"])\n",
    "    delivery_clean[\"delivery_time\"] = parse_datetime_flexible(delivery_clean[\"delivery_time\"])\n",
    "    delivery_clean[\"accept_time\"] = parse_datetime_flexible(delivery_clean[\"accept_time\"])\n",
    "\n",
    "    if \"time_window_start\" in pickup_clean.columns:\n",
    "        pickup_clean[\"time_window_start\"] = parse_datetime_flexible(pickup_clean[\"time_window_start\"])\n",
    "        pickup_clean[\"time_window_end\"] = parse_datetime_flexible(pickup_clean[\"time_window_end\"])\n",
    "\n",
    "    pickup_clean = pickup_clean.dropna(subset=[\"pickup_time\", \"accept_time\"])\n",
    "    delivery_clean = delivery_clean.dropna(subset=[\"delivery_time\", \"accept_time\"])\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        pickup_clean,\n",
    "        delivery_clean,\n",
    "        on=\"order_id\",\n",
    "        how=\"inner\",\n",
    "        suffixes=[\"_pickup\", \"_delivery\"],\n",
    "    )\n",
    "\n",
    "    if \"source_city\" not in df_merged.columns:\n",
    "        if \"source_city_pickup\" in df_merged.columns:\n",
    "            df_merged[\"source_city\"] = df_merged[\"source_city_pickup\"]\n",
    "        elif \"source_city_delivery\" in df_merged.columns:\n",
    "            df_merged[\"source_city\"] = df_merged[\"source_city_delivery\"]\n",
    "\n",
    "    if len(df_merged) == 0:\n",
    "        raise ValueError(\"فشل في دمج البيانات - لا توجد مطابقات\")\n",
    "\n",
    "    df_merged[\"ETA_seconds\"] = (df_merged[\"delivery_time\"] - df_merged[\"pickup_time\"]).dt.total_seconds()\n",
    "    df_merged[\"ETA_hours\"] = df_merged[\"ETA_seconds\"] / 3600\n",
    "    df_merged[\"ETA_minutes\"] = df_merged[\"ETA_seconds\"] / 60\n",
    "\n",
    "    df_merged = df_merged[df_merged[\"ETA_hours\"] > 0]\n",
    "    df_merged = df_merged[\n",
    "        (df_merged[\"ETA_hours\"] >= 1 / 60) & (df_merged[\"ETA_hours\"] <= 168)\n",
    "    ]\n",
    "\n",
    "    Q1 = df_merged[\"ETA_hours\"].quantile(0.25)\n",
    "    Q3 = df_merged[\"ETA_hours\"].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_merged = df_merged[\n",
    "        (df_merged[\"ETA_hours\"] >= lower_bound) & (df_merged[\"ETA_hours\"] <= upper_bound)\n",
    "    ]\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. COMPREHENSIVE EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_eda(df_merged):\n",
    "    \"\"\"تحليل استكشافي شامل ومتقدم\"\"\"\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "    # 1. ETA Distribution\n",
    "    plt.subplot(3, 3, 1)\n",
    "    sns.histplot(df_merged['ETA_hours'], bins=50, kde=True, color='skyblue')\n",
    "    plt.axvline(df_merged['ETA_hours'].mean(), color='red', linestyle='--', label=f'المتوسط: {df_merged[\"ETA_hours\"].mean():.2f}h')\n",
    "    plt.axvline(df_merged['ETA_hours'].median(), color='green', linestyle='--', label=f'الوسيط: {df_merged[\"ETA_hours\"].median():.2f}h')\n",
    "    plt.title('توزيع ETA (بالساعات)')\n",
    "    plt.xlabel('ETA (ساعات)')\n",
    "    plt.ylabel('التكرار')\n",
    "    plt.legend()\n",
    "\n",
    "    # 2. ETA by City\n",
    "    plt.subplot(3, 3, 2)\n",
    "    sns.barplot(x='source_city', y='ETA_hours', data=df_merged, estimator=np.mean, ci='sd', capsize=.2, color='lightcoral')\n",
    "    plt.title('متوسط ETA حسب المدينة')\n",
    "    plt.xlabel('المدينة')\n",
    "    plt.ylabel('متوسط ETA (ساعات)')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # 3. ETA by Hour\n",
    "    plt.subplot(3, 3, 3)\n",
    "    df_merged[\"pickup_hour\"] = df_merged[\"pickup_time\"].dt.hour\n",
    "    sns.lineplot(x='pickup_hour', y='ETA_hours', data=df_merged, marker='o')\n",
    "    plt.title('متوسط ETA حسب ساعة الاستلام')\n",
    "    plt.xlabel('الساعة')\n",
    "    plt.ylabel('متوسط ETA (ساعات)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. ETA by Day of Week\n",
    "    plt.subplot(3, 3, 4)\n",
    "    df_merged[\"pickup_dayofweek\"] = df_merged[\"pickup_time\"].dt.day_name()\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    sns.barplot(x='pickup_dayofweek', y='ETA_hours', data=df_merged, order=day_order, color='lightgreen')\n",
    "    plt.title('متوسط ETA حسب اليوم')\n",
    "    plt.xlabel('اليوم')\n",
    "    plt.ylabel('متوسط ETA (ساعات)')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # 5. Box plot for ETA distribution by city\n",
    "    plt.subplot(3, 3, 5)\n",
    "    sns.boxplot(x='source_city', y='ETA_hours', data=df_merged, ax=plt.gca())\n",
    "    plt.title('توزيع ETA حسب المدينة (Box Plot)')\n",
    "    plt.suptitle('')\n",
    "\n",
    "    # 6. Correlation heatmap\n",
    "    plt.subplot(3, 3, 6)\n",
    "    numerical_cols = df_merged.select_dtypes(include=np.number).columns\n",
    "    correlation_matrix = df_merged[numerical_cols].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "    plt.title('مصفوفة الارتباط')\n",
    "    \n",
    "    # 9. Accept to Pickup time analysis\n",
    "    plt.subplot(3, 3, 9)\n",
    "    df_merged[\"accept_to_pickup_hours\"] = (df_merged[\"pickup_time\"] - df_merged[\"accept_time_pickup\"]).dt.total_seconds() / 3600\n",
    "    accept_to_pickup_clean = df_merged[(df_merged[\"accept_to_pickup_hours\"] >= 0) & (df_merged[\"accept_to_pickup_hours\"] <= 24)]\n",
    "    if len(accept_to_pickup_clean) > 0:\n",
    "        sns.histplot(accept_to_pickup_clean[\"accept_to_pickup_hours\"], bins=30, kde=True)\n",
    "        plt.title('توزيع الوقت من القبول للاستلام')\n",
    "        plt.xlabel('الوقت (ساعات)')\n",
    "        plt.ylabel('التكرار')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ADVANCED FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_feature_engineering(df_merged):\n",
    "    \"\"\"هندسة خصائص متقدمة ومحسنة\"\"\"\n",
    "    df_features = df_merged.copy()\n",
    "\n",
    "    # Time-based features\n",
    "    df_features[\"pickup_hour\"] = df_features[\"pickup_time\"].dt.hour\n",
    "    df_features[\"pickup_minute\"] = df_features[\"pickup_time\"].dt.minute\n",
    "    df_features[\"pickup_dayofweek\"] = df_features[\"pickup_time\"].dt.dayofweek\n",
    "    df_features[\"pickup_day\"] = df_features[\"pickup_time\"].dt.day\n",
    "    df_features[\"pickup_month\"] = df_features[\"pickup_time\"].dt.month\n",
    "    df_features[\"pickup_quarter\"] = df_features[\"pickup_time\"].dt.quarter\n",
    "\n",
    "    # Weekend and business hours\n",
    "    df_features[\"is_weekend\"] = df_features[\"pickup_dayofweek\"].isin([5, 6]).astype(int)\n",
    "    df_features[\"is_business_hours\"] = ((df_features[\"pickup_hour\"] >= 9) & (df_features[\"pickup_hour\"] <= 17)).astype(int)\n",
    "\n",
    "    # Cyclical encoding for time features\n",
    "    df_features[\"hour_sin\"] = np.sin(2 * np.pi * df_features[\"pickup_hour\"] / 24)\n",
    "    df_features[\"hour_cos\"] = np.cos(2 * np.pi * df_features[\"pickup_hour\"] / 24)\n",
    "\n",
    "    # Accept to pickup time\n",
    "    df_features[\"accept_to_pickup_hours\"] = (df_features[\"pickup_time\"] - df_features[\"accept_time_pickup\"]).dt.total_seconds() / 3600\n",
    "    df_features[\"accept_to_pickup_hours\"] = df_features[\"accept_to_pickup_hours\"].clip(0, 48)\n",
    "\n",
    "    # Distance features\n",
    "    if all(col in df_features.columns for col in [\"lng_pickup\", \"lat_pickup\", \"lng_delivery\", \"lat_delivery\"]):\n",
    "        def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "            R = 6371\n",
    "            lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "            dlat = lat2_rad - lat1_rad\n",
    "            dlon = lon2_rad - lon1_rad\n",
    "            a = np.sin(dlat / 2) ** 2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2) ** 2\n",
    "            c = 2 * np.arcsin(np.sqrt(a))\n",
    "            return R * c\n",
    "\n",
    "        df_features[\"distance_km\"] = haversine_distance(\n",
    "            df_features[\"lat_pickup\"], df_features[\"lng_pickup\"], df_features[\"lat_delivery\"], df_features[\"lng_delivery\"]\n",
    "        )\n",
    "\n",
    "    # Categorical encoding\n",
    "    label_encoders = {}\n",
    "    for col in df_features.select_dtypes(include=['object']).columns:\n",
    "        if df_features[col].nunique() > 1:\n",
    "            le = LabelEncoder()\n",
    "            df_features[f\"{col}_encoded\"] = le.fit_transform(df_features[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            \n",
    "    # Aggregated features\n",
    "    if \"courier_id_pickup\" in df_features.columns:\n",
    "        courier_stats = df_features.groupby(\"courier_id_pickup\")[\"ETA_hours\"].agg([\"mean\", \"std\"]).add_prefix(\"courier_eta_\")\n",
    "        df_features = df_features.join(courier_stats, on=\"courier_id_pickup\")\n",
    "    \n",
    "    # Select relevant features\n",
    "    feature_columns = [\n",
    "        'pickup_hour', 'pickup_dayofweek', 'is_weekend', 'accept_to_pickup_hours',\n",
    "        'hour_sin', 'hour_cos'\n",
    "    ]\n",
    "    if 'distance_km' in df_features.columns:\n",
    "        feature_columns.append('distance_km')\n",
    "    if 'source_city_encoded' in df_features.columns:\n",
    "        feature_columns.append('source_city_encoded')\n",
    "    if 'courier_eta_mean' in df_features.columns:\n",
    "        feature_columns.extend(['courier_eta_mean', 'courier_eta_std'])\n",
    "        \n",
    "    feature_columns = [col for col in feature_columns if col in df_features.columns]\n",
    "\n",
    "    return df_features, feature_columns, label_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ADVANCED OUTLIER DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_outlier_detection(df_features, feature_columns):\n",
    "    \"\"\"كشف وإزالة الـ outliers بطرق متقدمة\"\"\"\n",
    "    df_clean = df_features.copy()\n",
    "    initial_count = len(df_clean)\n",
    "\n",
    "    # 1. Z-score for ETA\n",
    "    z_scores = np.abs(stats.zscore(df_clean[\"ETA_hours\"]))\n",
    "    df_clean = df_clean[z_scores < 3]\n",
    "\n",
    "    # 2. Isolation Forest for multivariate outliers\n",
    "    numerical_features = [col for col in feature_columns if df_clean[col].dtype in ['int64', 'float64'] and not df_clean[col].isna().all()]\n",
    "    if len(numerical_features) > 1:\n",
    "        iso_data = df_clean[numerical_features].fillna(df_clean[numerical_features].median())\n",
    "        iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "        outlier_labels = iso_forest.fit_predict(iso_data)\n",
    "        df_clean = df_clean[outlier_labels != -1]\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MODELING DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_advanced_modeling_data(df_features, feature_columns):\n",
    "    \"\"\"تحضير بيانات النمذجة المتقدمة\"\"\"\n",
    "    df_modeling = df_features.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in feature_columns:\n",
    "        if df_modeling[col].dtype in ['int64', 'float64']:\n",
    "            df_modeling[col].fillna(df_modeling[col].median(), inplace=True)\n",
    "        else:\n",
    "            df_modeling[col].fillna(df_modeling[col].mode()[0], inplace=True)\n",
    "\n",
    "    df_modeling = df_modeling.dropna(subset=[\"ETA_hours\"])\n",
    "    \n",
    "    available_features = [col for col in feature_columns if col in df_modeling.columns]\n",
    "    X = df_modeling[available_features]\n",
    "    y = df_modeling[\"ETA_hours\"]\n",
    "\n",
    "    non_numeric_cols = X.select_dtypes(exclude=np.number).columns\n",
    "    if not non_numeric_cols.empty:\n",
    "        X = X.drop(columns=non_numeric_cols)\n",
    "        available_features = [c for c in available_features if c not in non_numeric_cols]\n",
    "\n",
    "    # Stratified split\n",
    "    eta_bins = pd.qcut(y, q=5, labels=False, duplicates='drop')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=eta_bins\n",
    "    )\n",
    "\n",
    "    # Scaling\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert scaled arrays back to DataFrame to preserve column names\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, available_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MODEL TRAINING & TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_improved_models(X_train, X_test, y_train, y_test, feature_columns):\n",
    "    \"\"\"تدريب نماذج محسنة مع ضبط المعاملات\"\"\"\n",
    "    models_results = {}\n",
    "\n",
    "    # Models setup\n",
    "    models = {\n",
    "        \"Ridge\": (GridSearchCV(Ridge(random_state=42), {'alpha': [0.1, 1.0, 10.0]}, cv=5, scoring='r2'), {}),\n",
    "        \"RandomForest\": (RandomizedSearchCV(RandomForestRegressor(random_state=42), {'n_estimators': [100, 150], 'max_depth': [10, 20]}, n_iter=2, cv=3, scoring='r2', random_state=42), {}),\n",
    "        \"GradientBoosting\": (GridSearchCV(GradientBoostingRegressor(random_state=42), {'n_estimators': [100], 'learning_rate': [0.1]}, cv=3, scoring='r2'), {}),\n",
    "        \"CatBoost\": (cb.CatBoostRegressor(iterations=200, depth=6, learning_rate=0.1, loss_function='RMSE', verbose=False, random_state=42), {})\n",
    "    }\n",
    "\n",
    "    for name, (model, params) in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        best_estimator = model.best_estimator_ if hasattr(model, 'best_estimator_') else model\n",
    "        \n",
    "        pred_train = best_estimator.predict(X_train)\n",
    "        pred_test = best_estimator.predict(X_test)\n",
    "\n",
    "        feature_imp = None\n",
    "        if hasattr(best_estimator, 'feature_importances_'):\n",
    "            feature_imp = pd.DataFrame({'feature': feature_columns, 'importance': best_estimator.feature_importances_}).sort_values('importance', ascending=False)\n",
    "\n",
    "        models_results[name] = {\n",
    "            'model': best_estimator,\n",
    "            'train_mae': mean_absolute_error(y_train, pred_train),\n",
    "            'test_mae': mean_absolute_error(y_test, pred_test),\n",
    "            'train_rmse': np.sqrt(mean_squared_error(y_train, pred_train)),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y_test, pred_test)),\n",
    "            'train_r2': r2_score(y_train, pred_train),\n",
    "            'test_r2': r2_score(y_test, pred_test),\n",
    "            'best_params': model.best_params_ if hasattr(model, 'best_params_') else model.get_params(),\n",
    "            'feature_importance': feature_imp\n",
    "        }\n",
    "\n",
    "    return models_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ENSEMBLE METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_model(models_results, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"إنشاء نموذج مجمع من أفضل النماذج\"\"\"\n",
    "    model_scores = sorted([(name, res['test_r2']) for name, res in models_results.items()], key=lambda x: x[1], reverse=True)\n",
    "    top_models = model_scores[:3]\n",
    "    \n",
    "    ensemble_train_preds = []\n",
    "    ensemble_test_preds = []\n",
    "    weights = [score for _, score in top_models]\n",
    "    weights = np.array(weights) / np.sum(weights)\n",
    "    \n",
    "    component_model_names = []\n",
    "    for i, (name, _) in enumerate(top_models):\n",
    "        model = models_results[name]['model']\n",
    "        ensemble_train_preds.append(model.predict(X_train))\n",
    "        ensemble_test_preds.append(model.predict(X_test))\n",
    "        component_model_names.append(name)\n",
    "\n",
    "    ensemble_train_pred = np.average(ensemble_train_preds, axis=0, weights=weights)\n",
    "    ensemble_test_pred = np.average(ensemble_test_preds, axis=0, weights=weights)\n",
    "\n",
    "    ensemble_results = {\n",
    "        'model': 'Weighted Ensemble',\n",
    "        'train_mae': mean_absolute_error(y_train, ensemble_train_pred),\n",
    "        'test_mae': mean_absolute_error(y_test, ensemble_test_pred),\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, ensemble_train_pred)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, ensemble_test_pred)),\n",
    "        'train_r2': r2_score(y_train, ensemble_train_pred),\n",
    "        'test_r2': r2_score(y_test, ensemble_test_pred),\n",
    "        'weights': weights.tolist(),\n",
    "        'component_models': component_model_names\n",
    "    }\n",
    "    models_results['Ensemble'] = ensemble_results\n",
    "    return models_results, ensemble_train_pred, ensemble_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ADVANCED MODEL EVALUATION (Output Enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_model_evaluation(\n",
    "    models_results, y_test, best_test_pred\n",
    "):\n",
    "    \"\"\"تقييم متقدم للنماذج مع رسوم بيانية\"\"\"\n",
    "    comparison_data = []\n",
    "    for model_name, results in models_results.items():\n",
    "        comparison_data.append(\n",
    "            {\n",
    "                \"النموذج\": model_name,\n",
    "                \"Test MAE\": results[\"test_mae\"],\n",
    "                \"Test RMSE\": results[\"test_rmse\"],\n",
    "                \"Test R²\": results[\"test_r2\"],\n",
    "                \"Train R²\": results[\"train_r2\"],\n",
    "                \"Overfitting\": results[\"train_r2\"] - results[\"test_r2\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data).sort_values(\"Test R²\", ascending=False)\n",
    "\n",
    "    print(\"\\n📊 مقارنة شاملة للنماذج:\")\n",
    "    print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "    best_model_name = comparison_df.iloc[0][\"النموذج\"]\n",
    "    print(f\"\\n🏆 أفضل نموذج: {best_model_name}\")\n",
    "    print(f\"   Test R²: {comparison_df.iloc[0]['Test R²']:.4f}\")\n",
    "    print(f\"   Test MAE: {comparison_df.iloc[0]['Test MAE']:.4f} ساعة\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "    # 1. Model comparison chart (R²)\n",
    "    plt.subplot(2, 3, 1)\n",
    "    sns.barplot(x='Test R²', y='النموذج', data=comparison_df, palette='viridis')\n",
    "    plt.title('مقارنة النماذج - R² Score')\n",
    "    plt.xlabel('R² Score')\n",
    "\n",
    "    # 2. Actual vs Predicted\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.scatter(y_test, best_test_pred, alpha=0.5, label='التنبؤات')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='الخط المثالي')\n",
    "    plt.xlabel(\"ETA الفعلي (ساعات)\")\n",
    "    plt.ylabel(\"ETA المتوقع (ساعات)\")\n",
    "    plt.title(f\"الفعلي مقابل المتوقع - {best_model_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 3. Residuals plot\n",
    "    plt.subplot(2, 3, 5)\n",
    "    residuals = y_test - best_test_pred\n",
    "    sns.scatterplot(x=best_test_pred, y=residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='red', linestyle='--')\n",
    "    plt.xlabel(\"ETA المتوقع (ساعات)\")\n",
    "    plt.ylabel(\"البواقي (ساعات)\")\n",
    "    plt.title(\"رسم البواقي\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 4. Feature importance\n",
    "    plt.subplot(2, 3, 6)\n",
    "    if 'feature_importance' in models_results[best_model_name] and models_results[best_model_name]['feature_importance'] is not None:\n",
    "        importance_df = models_results[best_model_name]['feature_importance'].head(10)\n",
    "        sns.barplot(x='importance', y='feature', data=importance_df, palette='plasma')\n",
    "        plt.title('أهم 10 خصائص')\n",
    "        plt.xlabel('الأهمية')\n",
    "        plt.ylabel('الخاصية')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Error analysis\n",
    "    mae = mean_absolute_error(y_test, best_test_pred)\n",
    "    print(\"\\n🔍 تحليل الأخطاء:\")\n",
    "    print(f\"   متوسط الخطأ المطلق: {mae:.2f} ساعة = {mae*60:.0f} دقيقة\")\n",
    "    for bound in [0.5, 1.0, 2.0]:\n",
    "        within_bound = (np.abs(residuals) <= bound).mean() * 100\n",
    "        print(f\"   التنبؤات ضمن ±{bound} ساعة: {within_bound:.1f}%\")\n",
    "        \n",
    "    return comparison_df, best_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MAIN EXECUTION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline():\n",
    "    \"\"\"التدفق الرئيسي لتدريب النموذج\"\"\"\n",
    "    try:\n",
    "        df_pickup, df_delivery = load_data()\n",
    "        df_merged = advanced_data_preparation(df_pickup, df_delivery)\n",
    "        df_merged_eda = comprehensive_eda(df_merged)\n",
    "        df_features, feature_columns, label_encoders = advanced_feature_engineering(df_merged_eda)\n",
    "        df_clean = advanced_outlier_detection(df_features, feature_columns)\n",
    "        X_train, X_test, y_train, y_test, scaler, available_features = prepare_advanced_modeling_data(df_clean, feature_columns)\n",
    "        models_results = train_improved_models(X_train, X_test, y_train, y_test, available_features)\n",
    "        models_results, ensemble_train_pred, ensemble_test_pred = create_ensemble_model(models_results, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # The evaluation function is called here, and it's the only one with prints\n",
    "        comparison_df, best_model_name = advanced_model_evaluation(models_results, y_test, ensemble_test_pred)\n",
    "        \n",
    "        best_model = None\n",
    "        if best_model_name == 'Ensemble':\n",
    "            # Logic for creating a callable ensemble model object would go here\n",
    "            pass\n",
    "        else:\n",
    "            best_model = models_results[best_model_name]['model']\n",
    "            \n",
    "        return {\n",
    "            \"model\": best_model,\n",
    "            \"scaler\": scaler,\n",
    "            \"feature_columns\": available_features,\n",
    "            \"label_encoders\": label_encoders,\n",
    "            \"comparison_results\": comparison_df,\n",
    "            \"models_results\": models_results,\n",
    "            \"best_model_name\": best_model_name,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. PRODUCTION UTILITIES & EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_for_production(results, filepath_prefix=\"delivery_prediction_model\"):\n",
    "    \"\"\"حفظ النموذج للاستخدام في الإنتاج\"\"\"\n",
    "    model_components = {\n",
    "        \"model\": results[\"model\"],\n",
    "        \"scaler\": results[\"scaler\"],\n",
    "        \"feature_columns\": results[\"feature_columns\"],\n",
    "        \"label_encoders\": results[\"label_encoders\"],\n",
    "        \"model_name\": results[\"best_model_name\"],\n",
    "    }\n",
    "    with open(f\"{filepath_prefix}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model_components, f)\n",
    "    results[\"comparison_results\"].to_csv(f\"{filepath_prefix}_performance.csv\", index=False)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # To run the full pipeline and see the evaluation output:\n",
    "    # final_results = main_pipeline()\n",
    "    # if final_results:\n",
    "    #    save_model_for_production(final_results)\n",
    "    pass # The execution is meant to be done cell by cell in the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
