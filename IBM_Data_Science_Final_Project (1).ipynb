{
      "cell_type": "markdown",
      "source": [
        "# Improved Delivery Time Prediction Model\n",
        "\n",
        "**Objective:** Predict ETA (Estimated Time of Arrival) with high accuracy.\n",
        "ETA = Delivery Time - Pickup Time\n",
        "\n",
        "**Enhancements:**\n",
        "*   Improved data cleaning\n",
        "*   Outlier removal\n",
        "*   Comprehensive exploratory analysis\n",
        "*   Advanced feature engineering\n",
        "*   Model optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    cross_val_score,\n",
        "    GridSearchCV,\n",
        "    RandomizedSearchCV,\n",
        ")\n",
        "from sklearn.linear_model import Ridge, ElasticNet, LassoCV\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor,\n",
        "    GradientBoostingRegressor,\n",
        "    IsolationForest,\n",
        ")\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import catboost as cb\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set plotting style\n",
        "try:\n",
        "    plt.style.use(\"seaborn-v0_8\")\n",
        "except:\n",
        "    plt.style.use(\"seaborn\")\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
        "plt.rcParams[\"font.size\"] = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. DATA LOADING AND EXPLORATION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    \"\"\"Load pickup and delivery data from all cities with better error handling\"\"\"\n",
        "    cities = [\"cq\", \"sh\", \"hz\", \"jl\", \"yt\"]\n",
        "    pickup_dfs = []\n",
        "    delivery_dfs = []\n",
        "\n",
        "    for city in cities:\n",
        "        try:\n",
        "            # Load pickup data\n",
        "            pickup_df = pd.read_csv(f\"Pickup Five Cities Datasets/pickup_{city}.csv\")\n",
        "            pickup_df[\"source_city\"] = city\n",
        "            pickup_dfs.append(pickup_df)\n",
        "\n",
        "            # Load delivery data\n",
        "            delivery_df = pd.read_csv(\n",
        "                f\"Delivery Five Cities Datasets/delivery_{city}.csv\"\n",
        "            )\n",
        "            delivery_df[\"source_city\"] = city\n",
        "            delivery_dfs.append(delivery_df)\n",
        "\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    if not pickup_dfs or not delivery_dfs:\n",
        "        raise ValueError(\"No data was loaded successfully\")\n",
        "\n",
        "    # Combine data\n",
        "    df_pickup = pd.concat(pickup_dfs, ignore_index=True)\n",
        "    df_delivery = pd.concat(delivery_dfs, ignore_index=True)\n",
        "\n",
        "    return df_pickup, df_delivery\n",
        "\n",
        "def comprehensive_data_exploration(df_pickup, df_delivery):\n",
        "    \"\"\"Comprehensive data analysis\"\"\"\n",
        "    # Check data types and unique values\n",
        "    for col in df_pickup.columns:\n",
        "        if df_pickup[col].dtype == \"object\":\n",
        "            pass\n",
        "\n",
        "    return df_pickup, df_delivery"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ADVANCED DATA PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def advanced_data_preparation(df_pickup, df_delivery):\n",
        "    \"\"\"Advanced data preparation with comprehensive cleaning\"\"\"\n",
        "    # Create working copies\n",
        "    pickup_clean = df_pickup.copy()\n",
        "    delivery_clean = df_delivery.copy()\n",
        "\n",
        "    # Convert datetime columns with multiple format handling\n",
        "    def parse_datetime_flexible(date_series, formats=None):\n",
        "        \"\"\"Parse datetime with multiple format attempts\"\"\"\n",
        "        if formats is None:\n",
        "            formats = [\n",
        "                \"%m-%d %H:%M:%S\",\n",
        "                \"%Y-%m-%d %H:%M:%S\",\n",
        "                \"%d-%m-%Y %H:%M:%S\",\n",
        "                \"%m/%d %H:%M:%S\",\n",
        "                \"%Y/%m/%d %H:%M:%S\",\n",
        "            ]\n",
        "\n",
        "        parsed_series = pd.NaT\n",
        "        for fmt in formats:\n",
        "            try:\n",
        "                parsed_series = pd.to_datetime(date_series, format=fmt, errors=\"coerce\")\n",
        "                valid_count = parsed_series.notna().sum()\n",
        "                if valid_count > 0:\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if parsed_series.isna().all():\n",
        "            pass\n",
        "\n",
        "        return parsed_series\n",
        "\n",
        "    # Parse pickup times\n",
        "    pickup_clean[\"pickup_time\"] = parse_datetime_flexible(pickup_clean[\"pickup_time\"])\n",
        "    pickup_clean[\"accept_time\"] = parse_datetime_flexible(pickup_clean[\"accept_time\"])\n",
        "\n",
        "    # Parse delivery times\n",
        "    delivery_clean[\"delivery_time\"] = parse_datetime_flexible(\n",
        "        delivery_clean[\"delivery_time\"]\n",
        "    )\n",
        "    delivery_clean[\"accept_time\"] = parse_datetime_flexible(\n",
        "        delivery_clean[\"accept_time\"]\n",
        "    )\n",
        "\n",
        "    # Parse time windows\n",
        "    if \"time_window_start\" in pickup_clean.columns:\n",
        "        pickup_clean[\"time_window_start\"] = parse_datetime_flexible(\n",
        "            pickup_clean[\"time_window_start\"]\n",
        "        )\n",
        "        pickup_clean[\"time_window_end\"] = parse_datetime_flexible(\n",
        "            pickup_clean[\"time_window_end\"]\n",
        "        )\n",
        "\n",
        "    # Remove records with invalid datetime parsing\n",
        "    initial_pickup_count = len(pickup_clean)\n",
        "    initial_delivery_count = len(delivery_clean)\n",
        "\n",
        "    pickup_clean = pickup_clean.dropna(subset=[\"pickup_time\", \"accept_time\"])\n",
        "    delivery_clean = delivery_clean.dropna(subset=[\"delivery_time\", \"accept_time\"])\n",
        "\n",
        "    # Merge data with multiple key attempts\n",
        "    # Try different merge strategies\n",
        "    merge_keys = [\"order_id\"]\n",
        "\n",
        "    # Add more potential merge keys if they exist\n",
        "    common_cols = set(pickup_clean.columns) & set(delivery_clean.columns)\n",
        "    potential_keys = [\n",
        "        col\n",
        "        for col in common_cols\n",
        "        if col not in [\"pickup_time\", \"delivery_time\", \"accept_time\"]\n",
        "    ]\n",
        "\n",
        "    # Primary merge on order_id\n",
        "    df_merged = pd.merge(\n",
        "        pickup_clean,\n",
        "        delivery_clean,\n",
        "        on=\"order_id\",\n",
        "        how=\"inner\",\n",
        "        suffixes=[\"_pickup\", \"_delivery\"],\n",
        "    )\n",
        "\n",
        "    # Ensure unified city column for downstream steps\n",
        "    if \"source_city\" not in df_merged.columns:\n",
        "        if \"source_city_pickup\" in df_merged.columns:\n",
        "            df_merged[\"source_city\"] = df_merged[\"source_city_pickup\"]\n",
        "        elif \"source_city_delivery\" in df_merged.columns:\n",
        "            df_merged[\"source_city\"] = df_merged[\"source_city_delivery\"]\n",
        "\n",
        "    if len(df_merged) == 0:\n",
        "        raise ValueError(\"Failed to merge data - no matches found\")\n",
        "\n",
        "    # Calculate ETA with validation\n",
        "    df_merged[\"ETA_seconds\"] = (\n",
        "        df_merged[\"delivery_time\"] - df_merged[\"pickup_time\"]\n",
        "    ).dt.total_seconds()\n",
        "    df_merged[\"ETA_hours\"] = df_merged[\"ETA_seconds\"] / 3600\n",
        "    df_merged[\"ETA_minutes\"] = df_merged[\"ETA_seconds\"] / 60\n",
        "\n",
        "    # Comprehensive data cleaning\n",
        "    # Remove invalid ETAs\n",
        "    initial_count = len(df_merged)\n",
        "\n",
        "    # Remove negative ETAs\n",
        "    df_merged = df_merged[df_merged[\"ETA_hours\"] > 0]\n",
        "\n",
        "    # Remove extreme ETAs (more than 7 days or less than 1 minute)\n",
        "    initial_count = len(df_merged)\n",
        "    df_merged = df_merged[\n",
        "        (df_merged[\"ETA_hours\"] >= 1 / 60)  # At least 1 minute\n",
        "        & (df_merged[\"ETA_hours\"] <= 168)  # At most 7 days\n",
        "    ]\n",
        "\n",
        "    # Remove statistical outliers using IQR method\n",
        "    Q1 = df_merged[\"ETA_hours\"].quantile(0.25)\n",
        "    Q3 = df_merged[\"ETA_hours\"].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    initial_count = len(df_merged)\n",
        "    df_merged = df_merged[\n",
        "        (df_merged[\"ETA_hours\"] >= lower_bound)\n",
        "        & (df_merged[\"ETA_hours\"] <= upper_bound)\n",
        "    ]\n",
        "\n",
        "    return df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. COMPREHENSIVE EDA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def comprehensive_eda(df):\n",
        "    \"\"\"\n",
        "    Comprehensive exploratory data analysis\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Basic info\n",
        "        # Missing values\n",
        "        missing_counts = df.isnull().sum()\n",
        "        if missing_counts.sum() > 0:\n",
        "            for col, count in missing_counts[missing_counts > 0].items():\n",
        "                pass\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        # Duplicates\n",
        "        duplicates = df.duplicated().sum()\n",
        "\n",
        "        # Text columns analysis\n",
        "        text_columns = df.select_dtypes(include=[\"object\"]).columns\n",
        "        if len(text_columns) > 0:\n",
        "            for col in text_columns:\n",
        "                unique_count = df[col].nunique()\n",
        "\n",
        "        # Numerical columns analysis\n",
        "        numerical_columns = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "        if len(numerical_columns) > 0:\n",
        "            # Basic statistics for key numerical columns\n",
        "            key_numerical = [\"ETA_hours\", \"distance_km\", \"avg_speed_kmh\"]\n",
        "            for col in key_numerical:\n",
        "                if col in df.columns:\n",
        "                    pass\n",
        "\n",
        "        # Try to create some basic plots (with error handling)\n",
        "        try:\n",
        "            # Set up the plotting style\n",
        "            plt.style.use(\"default\")\n",
        "            plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
        "            plt.rcParams[\"font.size\"] = 10\n",
        "\n",
        "            # Create a simple figure with subplots\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle(\"Basic Data Analysis\", fontsize=16, fontweight=\"bold\")\n",
        "\n",
        "            # Plot 1: ETA distribution\n",
        "            if \"ETA_hours\" in df.columns:\n",
        "                axes[0, 0].hist(\n",
        "                    df[\"ETA_hours\"].dropna(),\n",
        "                    bins=50,\n",
        "                    alpha=0.7,\n",
        "                    color=\"skyblue\",\n",
        "                    edgecolor=\"black\",\n",
        "                )\n",
        "                axes[0, 0].set_title(\"ETA Distribution (hours)\")\n",
        "                axes[0, 0].set_xlabel(\"ETA (hours)\")\n",
        "                axes[0, 0].set_ylabel(\"Frequency\")\n",
        "                axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot 2: Distance distribution\n",
        "            if \"distance_km\" in df.columns:\n",
        "                axes[0, 1].hist(\n",
        "                    df[\"distance_km\"].dropna(),\n",
        "                    bins=50,\n",
        "                    alpha=0.7,\n",
        "                    color=\"lightgreen\",\n",
        "                    edgecolor=\"black\",\n",
        "                )\n",
        "                axes[0, 1].set_title(\"Distance Distribution (km)\")\n",
        "                axes[0, 1].set_xlabel(\"Distance (km)\")\n",
        "                axes[0, 1].set_ylabel(\"Frequency\")\n",
        "                axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot 3: City distribution\n",
        "            if \"source_city\" in df.columns:\n",
        "                city_counts = df[\"source_city\"].value_counts()\n",
        "                axes[1, 0].bar(\n",
        "                    city_counts.index, city_counts.values, color=\"lightcoral\", alpha=0.7\n",
        "                )\n",
        "                axes[1, 0].set_title(\"Order Distribution by City\")\n",
        "                axes[1, 0].set_xlabel(\"City\")\n",
        "                axes[1, 0].set_ylabel(\"Number of Orders\")\n",
        "                axes[1, 0].tick_params(axis=\"x\", rotation=45)\n",
        "                axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot 4: Time distribution\n",
        "            if \"pickup_hour\" in df.columns:\n",
        "                hour_counts = df[\"pickup_hour\"].value_counts().sort_index()\n",
        "                axes[1, 1].plot(\n",
        "                    hour_counts.index,\n",
        "                    hour_counts.values,\n",
        "                    marker=\"o\",\n",
        "                    color=\"orange\",\n",
        "                    linewidth=2,\n",
        "                )\n",
        "                axes[1, 1].set_title(\"Order Distribution by Hour\")\n",
        "                axes[1, 1].set_xlabel(\"Hour\")\n",
        "                axes[1, 1].set_ylabel(\"Number of Orders\")\n",
        "                axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            # Adjust layout safely\n",
        "            try:\n",
        "                plt.tight_layout()\n",
        "            except:\n",
        "                # If tight_layout fails, use a simple adjustment\n",
        "                plt.subplots_adjust(\n",
        "                    left=0.1, right=0.9, top=0.9, bottom=0.1, hspace=0.3, wspace=0.3\n",
        "                )\n",
        "\n",
        "            # Save the plot\n",
        "            plt.savefig(\"comprehensive_eda_plots.png\", dpi=300, bbox_inches=\"tight\")\n",
        "\n",
        "            # Close the plot to free memory\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as plot_error:\n",
        "            pass\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. ADVANCED FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def advanced_feature_engineering(df_merged):\n",
        "    \"\"\"Advanced and improved feature engineering\"\"\"\n",
        "    df_features = df_merged.copy()\n",
        "\n",
        "    # Time-based features\n",
        "    df_features[\"pickup_hour\"] = df_features[\"pickup_time\"].dt.hour\n",
        "    df_features[\"pickup_minute\"] = df_features[\"pickup_time\"].dt.minute\n",
        "    df_features[\"pickup_dayofweek\"] = df_features[\"pickup_time\"].dt.dayofweek\n",
        "    df_features[\"pickup_day\"] = df_features[\"pickup_time\"].dt.day\n",
        "    df_features[\"pickup_month\"] = df_features[\"pickup_time\"].dt.month\n",
        "    df_features[\"pickup_quarter\"] = df_features[\"pickup_time\"].dt.quarter\n",
        "\n",
        "    # Weekend and business hours\n",
        "    df_features[\"is_weekend\"] = df_features[\"pickup_dayofweek\"].isin([5, 6]).astype(int)\n",
        "    df_features[\"is_business_hours\"] = (\n",
        "        (df_features[\"pickup_hour\"] >= 9) & (df_features[\"pickup_hour\"] <= 17)\n",
        "    ).astype(int)\n",
        "    df_features[\"is_peak_hours\"] = (\n",
        "        (df_features[\"pickup_hour\"].isin([12, 13, 18, 19, 20]))\n",
        "    ).astype(int)\n",
        "    df_features[\"is_night\"] = (\n",
        "        (df_features[\"pickup_hour\"] >= 22) | (df_features[\"pickup_hour\"] <= 6)\n",
        "    ).astype(int)\n",
        "\n",
        "    # Cyclical encoding for time features\n",
        "    df_features[\"hour_sin\"] = np.sin(2 * np.pi * df_features[\"pickup_hour\"] / 24)\n",
        "    df_features[\"hour_cos\"] = np.cos(2 * np.pi * df_features[\"pickup_hour\"] / 24)\n",
        "    df_features[\"day_sin\"] = np.sin(2 * np.pi * df_features[\"pickup_dayofweek\"] / 7)\n",
        "    df_features[\"day_cos\"] = np.cos(2 * np.pi * df_features[\"pickup_dayofweek\"] / 7)\n",
        "    df_features[\"month_sin\"] = np.sin(2 * np.pi * df_features[\"pickup_month\"] / 12)\n",
        "    df_features[\"month_cos\"] = np.cos(2 * np.pi * df_features[\"pickup_month\"] / 12)\n",
        "\n",
        "    # Time window features\n",
        "    if \"time_window_start\" in df_features.columns:\n",
        "        df_features[\"has_time_window\"] = (\n",
        "            df_features[\"time_window_start\"].notna().astype(int)\n",
        "        )\n",
        "\n",
        "        # Calculate time window duration\n",
        "        time_window_mask = df_features[\"has_time_window\"] == 1\n",
        "        if time_window_mask.sum() > 0:\n",
        "            df_features.loc[time_window_mask, \"time_window_duration\"] = (\n",
        "                df_features.loc[time_window_mask, \"time_window_end\"]\n",
        "                - df_features.loc[time_window_mask, \"time_window_start\"]\n",
        "            ).dt.total_seconds() / 3600\n",
        "\n",
        "            # Time until window start\n",
        "            df_features.loc[time_window_mask, \"time_to_window_start\"] = (\n",
        "                df_features.loc[time_window_mask, \"time_window_start\"]\n",
        "                - df_features.loc[time_window_mask, \"pickup_time\"]\n",
        "            ).dt.total_seconds() / 3600\n",
        "\n",
        "    # Accept to pickup time\n",
        "    df_features[\"accept_to_pickup_hours\"] = (\n",
        "        df_features[\"pickup_time\"] - df_features[\"accept_time_pickup\"]\n",
        "    ).dt.total_seconds() / 3600\n",
        "\n",
        "    # Clean accept to pickup time (remove negative and extreme values)\n",
        "    df_features[\"accept_to_pickup_hours\"] = df_features[\"accept_to_pickup_hours\"].clip(\n",
        "        0, 48\n",
        "    )\n",
        "\n",
        "    # Distance features (if coordinates available)\n",
        "    if all(\n",
        "        col in df_features.columns\n",
        "        for col in [\"lng_pickup\", \"lat_pickup\", \"lng_delivery\", \"lat_delivery\"]\n",
        "    ):\n",
        "        # Calculate haversine distance (more efficient than geodesic for large datasets)\n",
        "        def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "            R = 6371  # Earth's radius in km\n",
        "\n",
        "            lat1_rad = np.radians(lat1)\n",
        "            lon1_rad = np.radians(lon1)\n",
        "            lat2_rad = np.radians(lat2)\n",
        "            lon2_rad = np.radians(lon2)\n",
        "\n",
        "            dlat = lat2_rad - lat1_rad\n",
        "            dlon = lon2_rad - lon1_rad\n",
        "\n",
        "            a = (\n",
        "                np.sin(dlat / 2) ** 2\n",
        "                + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2) ** 2\n",
        "            )\n",
        "            c = 2 * np.arcsin(np.sqrt(a))\n",
        "\n",
        "            return R * c\n",
        "\n",
        "        df_features[\"distance_km\"] = haversine_distance(\n",
        "            df_features[\"lat_pickup\"],\n",
        "            df_features[\"lng_pickup\"],\n",
        "            df_features[\"lat_delivery\"],\n",
        "            df_features[\"lng_delivery\"],\n",
        "        )\n",
        "\n",
        "        # Distance categories\n",
        "        df_features[\"distance_category\"] = pd.cut(\n",
        "            df_features[\"distance_km\"],\n",
        "            bins=[0, 2, 5, 10, 20, np.inf],\n",
        "            labels=[\"very_short\", \"short\", \"medium\", \"long\", \"very_long\"],\n",
        "        ).astype(str)\n",
        "\n",
        "        # Speed estimate (distance / ETA)\n",
        "        df_features[\"avg_speed_kmh\"] = (\n",
        "            df_features[\"distance_km\"] / df_features[\"ETA_hours\"]\n",
        "        )\n",
        "        df_features[\"avg_speed_kmh\"] = df_features[\"avg_speed_kmh\"].replace(\n",
        "            [np.inf, -np.inf], np.nan\n",
        "        )\n",
        "        df_features[\"avg_speed_kmh\"] = df_features[\"avg_speed_kmh\"].clip(\n",
        "            0, 100\n",
        "        )  # Reasonable speed limits\n",
        "\n",
        "    # Categorical encoding\n",
        "    # Label encoding for high cardinality features\n",
        "    label_encoders = {}\n",
        "    high_cardinality_cols = []\n",
        "\n",
        "    for col in df_features.columns:\n",
        "        if df_features[col].dtype == \"object\":\n",
        "            unique_count = df_features[col].nunique()\n",
        "            if unique_count > 1:\n",
        "                le = LabelEncoder()\n",
        "                df_features[f\"{col}_encoded\"] = le.fit_transform(\n",
        "                    df_features[col].astype(str)\n",
        "                )\n",
        "                label_encoders[col] = le\n",
        "                high_cardinality_cols.append(f\"{col}_encoded\")\n",
        "\n",
        "    # Aggregated features by categorical variables\n",
        "    # Historical averages by courier, city, region\n",
        "    if \"courier_id_pickup\" in df_features.columns:\n",
        "        courier_stats = (\n",
        "            df_features.groupby(\"courier_id_pickup\")[\"ETA_hours\"]\n",
        "            .agg([\"mean\", \"std\", \"count\"])\n",
        "            .add_prefix(\"courier_\")\n",
        "        )\n",
        "        df_features = df_features.join(courier_stats, on=\"courier_id_pickup\")\n",
        "\n",
        "    if \"source_city\" in df_features.columns:\n",
        "        city_stats = (\n",
        "            df_features.groupby(\"source_city\")[\"ETA_hours\"]\n",
        "            .agg([\"mean\", \"std\", \"count\"])\n",
        "            .add_prefix(\"city_\")\n",
        "        )\n",
        "        df_features = df_features.join(city_stats, on=\"source_city\")\n",
        "\n",
        "    # Interaction features\n",
        "    if \"distance_km\" in df_features.columns:\n",
        "        df_features[\"distance_hour_interaction\"] = (\n",
        "            df_features[\"distance_km\"] * df_features[\"pickup_hour\"]\n",
        "        )\n",
        "        df_features[\"distance_weekend_interaction\"] = (\n",
        "            df_features[\"distance_km\"] * df_features[\"is_weekend\"]\n",
        "        )\n",
        "\n",
        "    df_features[\"hour_weekend_interaction\"] = (\n",
        "        df_features[\"pickup_hour\"] * df_features[\"is_weekend\"]\n",
        "    )\n",
        "\n",
        "    # Select relevant features for modeling\n",
        "    # Base features\n",
        "    feature_columns = [\n",
        "        \"pickup_hour\",\n",
        "        \"pickup_minute\",\n",
        "        \"pickup_dayofweek\",\n",
        "        \"pickup_day\",\n",
        "        \"pickup_month\",\n",
        "        \"pickup_quarter\",\n",
        "        \"is_weekend\",\n",
        "        \"is_business_hours\",\n",
        "        \"is_peak_hours\",\n",
        "        \"is_night\",\n",
        "        \"hour_sin\",\n",
        "        \"hour_cos\",\n",
        "        \"day_sin\",\n",
        "        \"day_cos\",\n",
        "        \"month_sin\",\n",
        "        \"month_cos\",\n",
        "        \"accept_to_pickup_hours\",\n",
        "    ]\n",
        "\n",
        "    # Add time window features if available\n",
        "    if \"has_time_window\" in df_features.columns:\n",
        "        feature_columns.extend(\n",
        "            [\"has_time_window\", \"time_window_duration\", \"time_to_window_start\"]\n",
        "        )\n",
        "\n",
        "    # Add distance features if available\n",
        "    if \"distance_km\" in df_features.columns:\n",
        "        feature_columns.extend(\n",
        "            [\n",
        "                \"distance_km\",\n",
        "                \"distance_category_encoded\",\n",
        "                \"avg_speed_kmh\",\n",
        "                \"distance_hour_interaction\",\n",
        "                \"distance_weekend_interaction\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Add GPS coordinates if available\n",
        "    if \"lng_pickup\" in df_features.columns:\n",
        "        feature_columns.extend(\n",
        "            [\"lng_pickup\", \"lat_pickup\", \"lng_delivery\", \"lat_delivery\"]\n",
        "        )\n",
        "\n",
        "    # Add aggregated features\n",
        "    aggregated_features = [\n",
        "        col\n",
        "        for col in df_features.columns\n",
        "        if (\n",
        "            (col.startswith(\"courier_\") or col.startswith(\"city_\"))\n",
        "            and col not in [\"source_city\", \"source_city_encoded\"]\n",
        "            and df_features[col].dtype in [\"int64\", \"float64\"]\n",
        "        )\n",
        "    ]\n",
        "    feature_columns.extend(aggregated_features)\n",
        "\n",
        "    # Add high cardinality encoded features\n",
        "    feature_columns.extend(high_cardinality_cols)\n",
        "\n",
        "    # Add interaction features\n",
        "    interaction_features = [col for col in df_features.columns if \"interaction\" in col]\n",
        "    feature_columns.extend(interaction_features)\n",
        "\n",
        "    # Remove duplicates and ensure all features exist\n",
        "    feature_columns = list(set(feature_columns))\n",
        "    feature_columns = [col for col in feature_columns if col in df_features.columns]\n",
        "\n",
        "    return df_features, feature_columns, label_encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. ADVANCED OUTLIER DETECTION AND REMOVAL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def advanced_outlier_detection(df_features, feature_columns):\n",
        "    \"\"\"Advanced outlier detection and removal\"\"\"\n",
        "    df_clean = df_features.copy()\n",
        "    initial_count = len(df_clean)\n",
        "\n",
        "    # 1. Statistical outliers in ETA using Z-score\n",
        "    z_scores = np.abs(stats.zscore(df_clean[\"ETA_hours\"]))\n",
        "    outlier_threshold = 3\n",
        "    eta_outliers = z_scores > outlier_threshold\n",
        "\n",
        "    df_clean = df_clean[~eta_outliers]\n",
        "\n",
        "    # 2. Multivariate outliers using Isolation Forest (for numerical features only)\n",
        "    numerical_features = []\n",
        "    for col in feature_columns:\n",
        "        if col in df_clean.columns and df_clean[col].dtype in [\"int64\", \"float64\"]:\n",
        "            if not df_clean[col].isna().all():\n",
        "                numerical_features.append(col)\n",
        "\n",
        "    if len(numerical_features) > 2:\n",
        "        # Prepare data for isolation forest\n",
        "        iso_data = df_clean[numerical_features].fillna(\n",
        "            df_clean[numerical_features].median()\n",
        "        )\n",
        "\n",
        "        # Use smaller contamination for conservative outlier removal\n",
        "        iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "        outlier_labels = iso_forest.fit_predict(iso_data)\n",
        "\n",
        "        multivariate_outliers = outlier_labels == -1\n",
        "        df_clean = df_clean[~multivariate_outliers]\n",
        "\n",
        "    # 3. Domain-specific outlier removal\n",
        "    # Remove unrealistic accept-to-pickup times\n",
        "    if \"accept_to_pickup_hours\" in df_clean.columns:\n",
        "        unrealistic_accept = (df_clean[\"accept_to_pickup_hours\"] < 0) | (\n",
        "            df_clean[\"accept_to_pickup_hours\"] > 72\n",
        "        )  # More than 3 days\n",
        "        df_clean = df_clean[~unrealistic_accept]\n",
        "\n",
        "    # Remove unrealistic speeds\n",
        "    if \"avg_speed_kmh\" in df_clean.columns:\n",
        "        unrealistic_speed = (df_clean[\"avg_speed_kmh\"] < 0.1) | (  # Too slow\n",
        "            df_clean[\"avg_speed_kmh\"] > 80\n",
        "        )  # Too fast for delivery\n",
        "        df_clean = df_clean[~unrealistic_speed]\n",
        "\n",
        "    return df_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. ADVANCED MODELING WITH HYPERPARAMETER TUNING"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_advanced_modeling_data(df_features, feature_columns):\n",
        "    \"\"\"Prepare data for advanced modeling\"\"\"\n",
        "    # Handle missing values intelligently\n",
        "    # Separate numerical and categorical features\n",
        "    numerical_features = []\n",
        "    categorical_features = []\n",
        "\n",
        "    for col in feature_columns:\n",
        "        if col in df_features.columns:\n",
        "            if df_features[col].dtype in [\"int64\", \"float64\"]:\n",
        "                numerical_features.append(col)\n",
        "            else:\n",
        "                categorical_features.append(col)\n",
        "\n",
        "    # Fill missing values\n",
        "    df_modeling = df_features.copy()\n",
        "\n",
        "    # Fill numerical features with median\n",
        "    for col in numerical_features:\n",
        "        if df_modeling[col].isna().sum() > 0:\n",
        "            median_val = df_modeling[col].median()\n",
        "            df_modeling[col].fillna(median_val, inplace=True)\n",
        "\n",
        "    # Fill categorical features with mode\n",
        "    for col in categorical_features:\n",
        "        if df_modeling[col].isna().sum() > 0:\n",
        "            mode_val = (\n",
        "                df_modeling[col].mode()[0]\n",
        "                if len(df_modeling[col].mode()) > 0\n",
        "                else \"unknown\"\n",
        "            )\n",
        "            df_modeling[col].fillna(mode_val, inplace=True)\n",
        "\n",
        "    # Ensure we have the target variable\n",
        "    df_modeling = df_modeling.dropna(subset=[\"ETA_hours\"])\n",
        "\n",
        "    # Final feature selection\n",
        "    available_features = [col for col in feature_columns if col in df_modeling.columns]\n",
        "\n",
        "    # Prepare features and target\n",
        "    X = df_modeling[available_features]\n",
        "    y = df_modeling[\"ETA_hours\"]\n",
        "\n",
        "    # Ensure X is all numeric for modeling\n",
        "    non_numeric_cols = [c for c in X.columns if X[c].dtype not in [\"int64\", \"float64\"]]\n",
        "    if non_numeric_cols:\n",
        "        X = X.drop(columns=non_numeric_cols)\n",
        "        available_features = [\n",
        "            c for c in available_features if c not in non_numeric_cols\n",
        "        ]\n",
        "\n",
        "    # Advanced train-test split with stratification on ETA bins\n",
        "    # Create ETA bins for stratification\n",
        "    eta_bins = pd.qcut(\n",
        "        y, q=5, labels=[\"very_fast\", \"fast\", \"medium\", \"slow\", \"very_slow\"]\n",
        "    )\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=eta_bins\n",
        "    )\n",
        "\n",
        "    # Feature scaling with robust scaler (less sensitive to outliers)\n",
        "    scaler = RobustScaler()\n",
        "\n",
        "    # Only scale numerical features\n",
        "    numerical_indices = [\n",
        "        i\n",
        "        for i, col in enumerate(available_features)\n",
        "        if X_train.iloc[:, i].dtype in [\"int64\", \"float64\"]\n",
        "    ]\n",
        "\n",
        "    if numerical_indices:\n",
        "        X_train_scaled = X_train.copy()\n",
        "        X_test_scaled = X_test.copy()\n",
        "\n",
        "        X_train_scaled.iloc[:, numerical_indices] = scaler.fit_transform(\n",
        "            X_train.iloc[:, numerical_indices]\n",
        "        )\n",
        "        X_test_scaled.iloc[:, numerical_indices] = scaler.transform(\n",
        "            X_test.iloc[:, numerical_indices]\n",
        "        )\n",
        "    else:\n",
        "        X_train_scaled = X_train\n",
        "        X_test_scaled = X_test\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, available_features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_improved_models(X_train, X_test, y_train, y_test, feature_columns):\n",
        "    \"\"\"Train improved models with hyperparameter tuning\"\"\"\n",
        "    models_results = {}\n",
        "\n",
        "    # 1. Ridge Regression with GridSearch\n",
        "    ridge_params = {\n",
        "        \"alpha\": [0.1, 1.0, 10.0, 100.0, 1000.0],\n",
        "        \"solver\": [\"auto\", \"svd\", \"cholesky\"],\n",
        "    }\n",
        "\n",
        "    ridge_grid = GridSearchCV(\n",
        "        Ridge(random_state=42),\n",
        "        ridge_params,\n",
        "        cv=5,\n",
        "        scoring=\"neg_mean_squared_error\",\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    ridge_grid.fit(X_train, y_train)\n",
        "\n",
        "    ridge_pred_train = ridge_grid.predict(X_train)\n",
        "    ridge_pred_test = ridge_grid.predict(X_test)\n",
        "\n",
        "    models_results[\"Ridge (Improved)\"] = {\n",
        "        \"model\": ridge_grid.best_estimator_,\n",
        "        \"train_mae\": mean_absolute_error(y_train, ridge_pred_train),\n",
        "        \"test_mae\": mean_absolute_error(y_test, ridge_pred_test),\n",
        "        \"train_rmse\": np.sqrt(mean_squared_error(y_train, ridge_pred_train)),\n",
        "        \"test_rmse\": np.sqrt(mean_squared_error(y_test, ridge_pred_test)),\n",
        "        \"train_r2\": r2_score(y_train, ridge_pred_train),\n",
        "        \"test_r2\": r2_score(y_test, ridge_pred_test),\n",
        "        \"best_params\": ridge_grid.best_params_,\n",
        "    }\n",
        "\n",
        "    # 2. Random Forest with RandomizedSearch for speed\n",
        "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "    rf_param_dist = {\n",
        "        \"n_estimators\": [100, 150, 200],\n",
        "        \"max_depth\": [10, 15, 20, None],\n",
        "        \"min_samples_split\": [5, 10, 20],\n",
        "        \"min_samples_leaf\": [2, 4, 8],\n",
        "        \"max_features\": [\"sqrt\", \"log2\"],\n",
        "        \"bootstrap\": [True],\n",
        "    }\n",
        "\n",
        "    # Subsample for tuning to speed up\n",
        "    max_tune_rows = 20000\n",
        "    if len(X_train) > max_tune_rows:\n",
        "        tune_idx = np.random.choice(len(X_train), size=max_tune_rows, replace=False)\n",
        "        X_tune = X_train.iloc[tune_idx]\n",
        "        y_tune = y_train.iloc[tune_idx]\n",
        "    else:\n",
        "        X_tune, y_tune = X_train, y_train\n",
        "\n",
        "    rf_rand = RandomizedSearchCV(\n",
        "        rf,\n",
        "        rf_param_dist,\n",
        "        n_iter=20,\n",
        "        cv=3,\n",
        "        scoring=\"neg_mean_squared_error\",\n",
        "        random_state=42,\n",
        "        n_jobs=1,  # safer on Windows to avoid heavy multi-proc nesting\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    rf_rand.fit(X_tune, y_tune)\n",
        "\n",
        "    # Refit best on full training set\n",
        "    best_rf = rf_rand.best_estimator_\n",
        "    best_rf.fit(X_train, y_train)\n",
        "\n",
        "    rf_pred_train = best_rf.predict(X_train)\n",
        "    rf_pred_test = best_rf.predict(X_test)\n",
        "\n",
        "    models_results[\"Random Forest (Improved)\"] = {\n",
        "        \"model\": best_rf,\n",
        "        \"train_mae\": mean_absolute_error(y_train, rf_pred_train),\n",
        "        \"test_mae\": mean_absolute_error(y_test, rf_pred_test),\n",
        "        \"train_rmse\": np.sqrt(mean_squared_error(y_train, rf_pred_train)),\n",
        "        \"test_rmse\": np.sqrt(mean_squared_error(y_test, rf_pred_test)),\n",
        "        \"train_r2\": r2_score(y_train, rf_pred_train),\n",
        "        \"test_r2\": r2_score(y_test, rf_pred_test),\n",
        "        \"best_params\": rf_rand.best_params_,\n",
        "        \"feature_importance\": pd.DataFrame(\n",
        "            {\n",
        "                \"feature\": feature_columns,\n",
        "                \"importance\": best_rf.feature_importances_,\n",
        "            }\n",
        "        ).sort_values(\"importance\", ascending=False),\n",
        "    }\n",
        "\n",
        "    # 3. Gradient Boosting with GridSearch\n",
        "    gb_params = {\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"learning_rate\": [0.05, 0.1, 0.2],\n",
        "        \"max_depth\": [3, 5, 7],\n",
        "        \"subsample\": [0.8, 0.9, 1.0],\n",
        "    }\n",
        "\n",
        "    gb_grid = GridSearchCV(\n",
        "        GradientBoostingRegressor(random_state=42),\n",
        "        gb_params,\n",
        "        cv=3,\n",
        "        scoring=\"neg_mean_squared_error\",\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    gb_grid.fit(X_train, y_train)\n",
        "\n",
        "    gb_pred_train = gb_grid.predict(X_train)\n",
        "    gb_pred_test = gb_grid.predict(X_test)\n",
        "\n",
        "    models_results[\"Gradient Boosting (Improved)\"] = {\n",
        "        \"model\": gb_grid.best_estimator_,\n",
        "        \"train_mae\": mean_absolute_error(y_train, gb_pred_train),\n",
        "        \"test_mae\": mean_absolute_error(y_test, gb_pred_test),\n",
        "        \"train_rmse\": np.sqrt(mean_squared_error(y_train, gb_pred_train)),\n",
        "        \"test_rmse\": np.sqrt(mean_squared_error(y_test, gb_pred_test)),\n",
        "        \"train_r2\": r2_score(y_train, gb_pred_train),\n",
        "        \"test_r2\": r2_score(y_test, gb_pred_test),\n",
        "        \"best_params\": gb_grid.best_params_,\n",
        "        \"feature_importance\": pd.DataFrame(\n",
        "            {\n",
        "                \"feature\": feature_columns,\n",
        "                \"importance\": gb_grid.best_estimator_.feature_importances_,\n",
        "            }\n",
        "        ).sort_values(\"importance\", ascending=False),\n",
        "    }\n",
        "\n",
        "    # 4. ElasticNet with GridSearch\n",
        "    elastic_params = {\n",
        "        \"alpha\": [0.1, 1.0, 10.0, 100.0],\n",
        "        \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "    }\n",
        "\n",
        "    elastic_grid = GridSearchCV(\n",
        "        ElasticNet(random_state=42, max_iter=2000),\n",
        "        elastic_params,\n",
        "        cv=5,\n",
        "        scoring=\"neg_mean_squared_error\",\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    elastic_grid.fit(X_train, y_train)\n",
        "\n",
        "    elastic_pred_train = elastic_grid.predict(X_train)\n",
        "    elastic_pred_test = elastic_grid.predict(X_test)\n",
        "\n",
        "    models_results[\"ElasticNet (Improved)\"] = {\n",
        "        \"model\": elastic_grid.best_estimator_,\n",
        "        \"train_mae\": mean_absolute_error(y_train, elastic_pred_train),\n",
        "        \"test_mae\": mean_absolute_error(y_test, elastic_pred_test),\n",
        "        \"train_rmse\": np.sqrt(mean_squared_error(y_train, elastic_pred_train)),\n",
        "        \"test_rmse\": np.sqrt(mean_squared_error(y_test, elastic_pred_test)),\n",
        "        \"train_r2\": r2_score(y_train, elastic_pred_train),\n",
        "        \"test_r2\": r2_score(y_test, elastic_pred_test),\n",
        "        \"best_params\": elastic_grid.best_params_,\n",
        "    }\n",
        "\n",
        "    # 4. Lasso Regression\n",
        "    lasso = LassoCV(cv=5, random_state=42, max_iter=2000)\n",
        "    lasso.fit(X_train, y_train)\n",
        "\n",
        "    lasso_pred_train = lasso.predict(X_train)\n",
        "    lasso_pred_test = lasso.predict(X_test)\n",
        "\n",
        "    models_results[\"Lasso Regression\"] = {\n",
        "        \"model\": lasso,\n",
        "        \"train_mae\": mean_absolute_error(y_train, lasso_pred_train),\n",
        "        \"test_mae\": mean_absolute_error(y_test, lasso_pred_test),\n",
        "        \"train_rmse\": np.sqrt(mean_squared_error(y_train, lasso_pred_train)),\n",
        "        \"test_rmse\": np.sqrt(mean_squared_error(y_test, lasso_pred_test)),\n",
        "        \"train_r2\": r2_score(y_train, lasso_pred_train),\n",
        "        \"test_r2\": r2_score(y_test, lasso_pred_test),\n",
        "        \"best_params\": {\"alpha\": lasso.alpha_},\n",
        "        \"feature_importance\": pd.DataFrame(\n",
        "            {\n",
        "                \"feature\": feature_columns,\n",
        "                \"importance\": np.abs(lasso.coef_),\n",
        "            }\n",
        "        ).sort_values(\"importance\", ascending=False),\n",
        "    }\n",
        "\n",
        "    # 5. K-Nearest Neighbors\n",
        "    # Use a smaller sample for KNN to speed up training\n",
        "    max_knn_rows = 10000\n",
        "    if len(X_train) > max_knn_rows:\n",
        "        knn_idx = np.random.choice(len(X_train), size=max_knn_rows, replace=False)\n",
        "        X_knn_train = X_train.iloc[knn_idx]\n",
        "        y_knn_train = y_train.iloc[knn_idx]\n",
        "    else:\n",
        "        X_knn_train, y_knn_train = X_train, y_train\n",
        "\n",
        "    knn = KNeighborsRegressor(n_neighbors=15, weights=\"distance\", n_jobs=1)\n",
        "    knn.fit(X_knn_train, y_knn_train)\n",
        "\n",
        "    knn_pred_train = knn.predict(X_train)\n",
        "    knn_pred_test = knn.predict(X_test)\n",
        "\n",
        "    models_results[\"K-Nearest Neighbors\"] = {\n",
        "        \"model\": knn,\n",
        "        \"train_mae\": mean_absolute_error(y_train, knn_pred_train),\n",
        "        \"test_mae\": mean_absolute_error(y_test, knn_pred_test),\n",
        "        \"train_rmse\": np.sqrt(mean_squared_error(y_train, knn_pred_train)),\n",
        "        \"test_rmse\": np.sqrt(mean_squared_error(y_test, knn_pred_test)),\n",
        "        \"train_r2\": r2_score(y_train, knn_pred_train),\n",
        "        \"test_r2\": r2_score(y_test, knn_pred_test),\n",
        "        \"best_params\": {\"n_neighbors\": 15, \"weights\": \"distance\"},\n",
        "        \"feature_importance\": None,  # KNN doesn't have feature importance\n",
        "    }\n",
        "\n",
        "    # 6. CatBoost Regressor\n",
        "    try:\n",
        "        catboost = cb.CatBoostRegressor(\n",
        "            iterations=500,\n",
        "            depth=6,\n",
        "            learning_rate=0.1,\n",
        "            loss_function=\"RMSE\",\n",
        "            verbose=False,\n",
        "            random_state=42,\n",
        "        )\n",
        "\n",
        "        catboost.fit(X_train, y_train)\n",
        "\n",
        "        catboost_pred_train = catboost.predict(X_train)\n",
        "        catboost_pred_test = catboost.predict(X_test)\n",
        "\n",
        "        models_results[\"CatBoost Regressor\"] = {\n",
        "            \"model\": catboost,\n",
        "            \"train_mae\": mean_absolute_error(y_train, catboost_pred_train),\n",
        "            \"test_mae\": mean_absolute_error(y_test, catboost_pred_test),\n",
        "            \"train_rmse\": np.sqrt(mean_squared_error(y_train, catboost_pred_train)),\n",
        "            \"test_rmse\": np.sqrt(mean_squared_error(y_test, catboost_pred_test)),\n",
        "            \"train_r2\": r2_score(y_train, catboost_pred_train),\n",
        "            \"test_r2\": r2_score(y_test, catboost_pred_test),\n",
        "            \"best_params\": {\"iterations\": 500, \"depth\": 6, \"learning_rate\": 0.1},\n",
        "            \"feature_importance\": pd.DataFrame(\n",
        "                {\n",
        "                    \"feature\": feature_columns,\n",
        "                    \"importance\": catboost.get_feature_importance(),\n",
        "                }\n",
        "            ).sort_values(\"importance\", ascending=False),\n",
        "        }\n",
        "\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    return models_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. ENSEMBLE METHODS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ensemble_model(models_results, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Create an ensemble model from the best models\"\"\"\n",
        "    # Select top 3 models based on test R²\n",
        "    model_scores = [\n",
        "        (name, results[\"test_r2\"]) for name, results in models_results.items()\n",
        "    ]\n",
        "    model_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_models = model_scores[:3]\n",
        "\n",
        "    # Create ensemble predictions\n",
        "    ensemble_train_preds = []\n",
        "    ensemble_test_preds = []\n",
        "    weights = []\n",
        "\n",
        "    for name, score in top_models:\n",
        "        model = models_results[name][\"model\"]\n",
        "        train_pred = model.predict(X_train)\n",
        "        test_pred = model.predict(X_test)\n",
        "\n",
        "        ensemble_train_preds.append(train_pred)\n",
        "        ensemble_test_preds.append(test_pred)\n",
        "        weights.append(score)  # Weight by R² score\n",
        "\n",
        "    # Normalize weights\n",
        "    weights = np.array(weights)\n",
        "    weights = weights / weights.sum()\n",
        "\n",
        "    # Weighted average ensemble\n",
        "    ensemble_train_pred = np.average(ensemble_train_preds, axis=0, weights=weights)\n",
        "    ensemble_test_pred = np.average(ensemble_test_preds, axis=0, weights=weights)\n",
        "\n",
        "    # Evaluate ensemble\n",
        "    ensemble_results = {\n",
        "        \"model\": \"Weighted Ensemble\",\n",
        "        \"train_mae\": mean_absolute_error(y_train, ensemble_train_pred),\n",
        "        \"test_mae\": mean_absolute_error(y_test, ensemble_test_pred),\n",
        "        \"train_rmse\": np.sqrt(mean_squared_error(y_train, ensemble_train_pred)),\n",
        "        \"test_rmse\": np.sqrt(mean_squared_error(y_test, ensemble_test_pred)),\n",
        "        \"train_r2\": r2_score(y_train, ensemble_train_pred),\n",
        "        \"test_r2\": r2_score(y_test, ensemble_test_pred),\n",
        "        \"weights\": weights,\n",
        "        \"component_models\": [name for name, _ in top_models],\n",
        "    }\n",
        "\n",
        "    models_results[\"Ensemble (Aggregated)\"] = ensemble_results\n",
        "\n",
        "    return models_results, ensemble_train_pred, ensemble_test_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. ADVANCED MODEL EVALUATION AND VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def advanced_model_evaluation(\n",
        "    models_results, y_train, y_test, best_train_pred, best_test_pred\n",
        "):\n",
        "    \"\"\"Advanced model evaluation with visualizations\"\"\"\n",
        "    # Create comprehensive comparison\n",
        "    comparison_data = []\n",
        "    for model_name, results in models_results.items():\n",
        "        comparison_data.append(\n",
        "            {\n",
        "                \"Model\": model_name,\n",
        "                \"Test MAE\": results[\"test_mae\"],\n",
        "                \"Test RMSE\": results[\"test_rmse\"],\n",
        "                \"Test R²\": results[\"test_r2\"],\n",
        "                \"Train R²\": results[\"train_r2\"],\n",
        "                \"Overfitting\": results[\"train_r2\"] - results[\"test_r2\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    comparison_df = comparison_df.sort_values(\"Test R²\", ascending=False)\n",
        "\n",
        "    print(\"\\n📊 Comprehensive Model Comparison:\")\n",
        "    print(comparison_df.round(4).to_string(index=False))\n",
        "\n",
        "    # Find best model\n",
        "    best_model_name = comparison_df.iloc[0][\"Model\"]\n",
        "    print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "    print(f\"   Test R²: {comparison_df.iloc[0]['Test R²']:.4f}\")\n",
        "    print(f\"   Test MAE: {comparison_df.iloc[0]['Test MAE']:.4f} hours\")\n",
        "    print(f\"   Overfitting: {comparison_df.iloc[0]['Overfitting']:.4f}\")\n",
        "\n",
        "    # Advanced visualization\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "    # 1. Model comparison chart\n",
        "    plt.subplot(2, 3, 1)\n",
        "    models = comparison_df[\"Model\"]\n",
        "    r2_scores = comparison_df[\"Test R²\"]\n",
        "    colors = [\"gold\" if x == best_model_name else \"lightblue\" for x in models]\n",
        "    bars = plt.bar(range(len(models)), r2_scores, color=colors, alpha=0.8)\n",
        "    plt.title(\"Model Comparison - R² Score\")\n",
        "    plt.ylabel(\"R² Score\")\n",
        "    plt.xticks(range(len(models)), models, rotation=45, ha=\"right\")\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, score in zip(bars, r2_scores):\n",
        "        plt.text(\n",
        "            bar.get_x() + bar.get_width() / 2,\n",
        "            bar.get_height() + 0.01,\n",
        "            f\"{score:.3f}\",\n",
        "            ha=\"center\",\n",
        "            va=\"bottom\",\n",
        "            fontweight=\"bold\",\n",
        "        )\n",
        "\n",
        "    # 2. MAE comparison\n",
        "    plt.subplot(2, 3, 2)\n",
        "    mae_scores = comparison_df[\"Test MAE\"]\n",
        "    colors = [\"gold\" if x == best_model_name else \"lightcoral\" for x in models]\n",
        "    bars = plt.bar(range(len(models)), mae_scores, color=colors, alpha=0.8)\n",
        "    plt.title(\"Model Comparison - MAE\")\n",
        "    plt.ylabel(\"MAE (hours)\")\n",
        "    plt.xticks(range(len(models)), models, rotation=45, ha=\"right\")\n",
        "\n",
        "    for bar, score in zip(bars, mae_scores):\n",
        "        plt.text(\n",
        "            bar.get_x() + bar.get_width() / 2,\n",
        "            bar.get_height() + 0.2,\n",
        "            f\"{score:.2f}\",\n",
        "            ha=\"center\",\n",
        "            va=\"bottom\",\n",
        "            fontweight=\"bold\",\n",
        "        )\n",
        "\n",
        "    # 3. Overfitting analysis\n",
        "    plt.subplot(2, 3, 3)\n",
        "    overfitting = comparison_df[\"Overfitting\"]\n",
        "    colors = [\"red\" if x > 0.1 else \"green\" for x in overfitting]\n",
        "    plt.bar(range(len(models)), overfitting, color=colors, alpha=0.7)\n",
        "    plt.title(\"Overfitting Analysis\")\n",
        "    plt.ylabel(\"Train R² - Test R²\")\n",
        "    plt.xticks(range(len(models)), models, rotation=45, ha=\"right\")\n",
        "    plt.axhline(\n",
        "        y=0.1, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Overfitting Threshold\"\n",
        "    )\n",
        "    plt.legend()\n",
        "\n",
        "    # 4. Actual vs Predicted scatter plot\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.scatter(y_test, best_test_pred, alpha=0.5, color=\"blue\", label=\"Predictions\")\n",
        "    plt.plot(\n",
        "        [y_test.min(), y_test.max()],\n",
        "        [y_test.min(), y_test.max()],\n",
        "        \"r--\",\n",
        "        lw=2,\n",
        "        label=\"Perfect Line\",\n",
        "    )\n",
        "    plt.xlabel(\"Actual ETA (hours)\")\n",
        "    plt.ylabel(\"Predicted ETA (hours)\")\n",
        "    plt.title(f\"Actual vs Predicted - {best_model_name}\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Calculate and display R²\n",
        "    r2 = r2_score(y_test, best_test_pred)\n",
        "    plt.text(\n",
        "        0.05,\n",
        "        0.95,\n",
        "        f\"R² = {r2:.3f}\",\n",
        "        transform=plt.gca().transAxes,\n",
        "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n",
        "    )\n",
        "\n",
        "    # 5. Residuals plot\n",
        "    plt.subplot(2, 3, 5)\n",
        "    residuals = y_test - best_test_pred\n",
        "    plt.scatter(best_test_pred, residuals, alpha=0.5)\n",
        "    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n",
        "    plt.xlabel(\"Predicted ETA (hours)\")\n",
        "    plt.ylabel(\"Residuals (hours)\")\n",
        "    plt.title(\"Residuals Plot\")\n",
        "\n",
        "    # 6. Feature importance (if available)\n",
        "    plt.subplot(2, 3, 6)\n",
        "    if \"feature_importance\" in models_results[best_model_name]:\n",
        "        importance_df = models_results[best_model_name][\"feature_importance\"].head(10)\n",
        "        plt.barh(range(len(importance_df)), importance_df[\"importance\"], alpha=0.7)\n",
        "        plt.yticks(range(len(importance_df)), importance_df[\"feature\"])\n",
        "        plt.xlabel(\"Importance\")\n",
        "        plt.title(\"Top 10 Features\")\n",
        "        plt.gca().invert_yaxis()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Error analysis\n",
        "    print(\"\\n🔍 Error Analysis:\")\n",
        "    mae = mean_absolute_error(y_test, best_test_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, best_test_pred))\n",
        "\n",
        "    print(f\"   Mean Absolute Error: {mae:.2f} hours = {mae*60:.0f} minutes\")\n",
        "    print(f\"   Root Mean Squared Error: {rmse:.2f} hours = {rmse*60:.0f} minutes\")\n",
        "\n",
        "    # Percentage of predictions within certain error bounds\n",
        "    error_bounds = [0.5, 1.0, 2.0, 4.0]  # hours\n",
        "    for bound in error_bounds:\n",
        "        within_bound = (np.abs(y_test - best_test_pred) <= bound).mean() * 100\n",
        "        print(f\"   Predictions within ±{bound} hours: {within_bound:.1f}%\")\n",
        "\n",
        "    return comparison_df, best_model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. MAIN EXECUTION PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline():\n",
        "    \"\"\"Main pipeline for model training\"\"\"\n",
        "    try:\n",
        "        # 1. Load data\n",
        "        df_pickup, df_delivery = load_data()\n",
        "\n",
        "        # 2. Comprehensive data exploration\n",
        "        df_pickup, df_delivery = comprehensive_data_exploration(df_pickup, df_delivery)\n",
        "\n",
        "        # 3. Advanced data preparation\n",
        "        df_merged = advanced_data_preparation(df_pickup, df_delivery)\n",
        "\n",
        "        # 4. Comprehensive EDA\n",
        "        df_merged = comprehensive_eda(df_merged)\n",
        "\n",
        "        # 5. Advanced feature engineering\n",
        "        df_features, feature_columns, label_encoders = advanced_feature_engineering(\n",
        "            df_merged\n",
        "        )\n",
        "\n",
        "        # 6. Advanced outlier detection\n",
        "        df_clean = advanced_outlier_detection(df_features, feature_columns)\n",
        "\n",
        "        # 7. Prepare modeling data\n",
        "        X_train, X_test, y_train, y_test, scaler, available_features = (\n",
        "            prepare_advanced_modeling_data(df_clean, feature_columns)\n",
        "        )\n",
        "\n",
        "        # 8. Train improved models\n",
        "        models_results = train_improved_models(\n",
        "            X_train, X_test, y_train, y_test, available_features\n",
        "        )\n",
        "\n",
        "        # 9. Create ensemble model\n",
        "        models_results, ensemble_train_pred, ensemble_test_pred = create_ensemble_model(\n",
        "            models_results, X_train, X_test, y_train, y_test\n",
        "        )\n",
        "\n",
        "        # 10. Advanced model evaluation\n",
        "        comparison_df, best_model_name = advanced_model_evaluation(\n",
        "            models_results, y_train, y_test, ensemble_train_pred, ensemble_test_pred\n",
        "        )\n",
        "\n",
        "        # 11. Create prediction function\n",
        "        best_model = models_results[best_model_name][\"model\"]\n",
        "        if best_model_name == \"Ensemble (Aggregated)\":\n",
        "            # For ensemble, we need to create a special prediction function\n",
        "            def ensemble_predict(X):\n",
        "                \"\"\"Ensemble prediction function\"\"\"\n",
        "                predictions = []\n",
        "                weights = models_results[best_model_name][\"weights\"]\n",
        "                component_models = models_results[best_model_name][\"component_models\"]\n",
        "\n",
        "                for i, model_name in enumerate(component_models):\n",
        "                    model = models_results[model_name][\"model\"]\n",
        "                    pred = model.predict(X)\n",
        "                    predictions.append(pred)\n",
        "\n",
        "                return np.average(predictions, axis=0, weights=weights)\n",
        "\n",
        "            # Create a wrapper object\n",
        "            class EnsembleWrapper:\n",
        "                def __init__(self, predict_func):\n",
        "                    self.predict = predict_func\n",
        "\n",
        "            best_model = EnsembleWrapper(ensemble_predict)\n",
        "\n",
        "        return {\n",
        "            \"model\": best_model,\n",
        "            \"scaler\": scaler,\n",
        "            \"feature_columns\": available_features,\n",
        "            \"label_encoders\": label_encoders,\n",
        "            \"comparison_results\": comparison_df,\n",
        "            \"models_results\": models_results,\n",
        "            \"best_model_name\": best_model_name,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. FINAL EXECUTION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        results = main_pipeline()\n",
        "        if results:\n",
        "            # Display final model performance summary\n",
        "            print(f\"\\n📊 Best Model Performance Summary:\")\n",
        "            print(f\"   Model: {results['best_model_name']}\")\n",
        "            best_results = results[\"models_results\"][results[\"best_model_name\"]]\n",
        "            print(f\"   Test R²: {best_results['test_r2']:.4f}\")\n",
        "            print(\n",
        "                f\"   Test MAE: {best_results['test_mae']:.2f} hours ({best_results['test_mae']*60:.0f} minutes)\"\n",
        "            )\n",
        "            print(\n",
        "                f\"   Test RMSE: {best_results['test_rmse']:.2f} hours ({best_results['test_rmse']*60:.0f} minutes)\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        pass"
      ]
    }
  ]
}
